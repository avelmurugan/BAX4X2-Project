{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fc22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "import codecs\n",
    "import os\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from pymongo import MongoClient\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import http.client, urllib.parse\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e381db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_page=72 for SF\n",
    "#num_page=33 for San Diego\n",
    "#num_page=61 for LA\n",
    "\n",
    "#function to scrape full craigslist page\n",
    "def scrape_full_page(region, num_page, fp_full_page):\n",
    "    #set region url\n",
    "    url='https://'+region+'.craigslist.org/search/cta?hasPic=1&purveyor=owner#search=1~gallery~0~0'\n",
    "    #get selenium web driver\n",
    "    browser=webdriver.Chrome('chromedriver')\n",
    "        #set browser wait\n",
    "    browser.implicitly_wait(1)\n",
    "            # Make first search\n",
    "    #get url\n",
    "    bc_search = browser.get(url)\n",
    "    #set 'next' button\n",
    "    button_path='//*[@id=\"search-toolbars-2\"]/div[1]/button[3]'\n",
    "    #loop through all pages\n",
    "    for i in range(0, num_page):\n",
    "        #wait ten seconds for page to load\n",
    "        time.sleep(10)\n",
    "        #download page to disk\n",
    "        with open(fp_full_page+\"/craigslist\"+str(i+1)+\".html\", \"w\", encoding = 'utf-8') as f:\n",
    "                    f.write(browser.page_source)\n",
    "        #sleep system\n",
    "        time.sleep(5)\n",
    "        #click 'next' button\n",
    "        browser.find_element(By.XPATH, button_path).click()\n",
    "    #quit browser\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509aeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get all individual pages\n",
    "def get_page_url(num_page, fp_full_page):\n",
    "    #create empty list\n",
    "    url_list=[]\n",
    "    #loop through all individual pages\n",
    "    for i in range(0,num_page):\n",
    "        #open full page object\n",
    "        htmlfile=open(fp_full_page+\"/craigslist\"+str(i+1)+\".html\", \"r\").read()\n",
    "        #Convert to soup object\n",
    "        soup=BeautifulSoup(htmlfile, 'html.parser')\n",
    "        #loop through all href\n",
    "        for i in soup.find_all('a',{'class':'titlestring'}):\n",
    "            #append to url list\n",
    "            url_list.append(i.get('href'))\n",
    "    #return url list\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1736dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download indiv pages\n",
    "def get_individual_page(num_page, fp_indiv_page, fp_full_page):\n",
    "    #pause loop for page to load\n",
    "    time.sleep(5)\n",
    "    #get urls\n",
    "    url_list=get_page_url(num_page, fp_full_page)\n",
    "    #loop through URLS\n",
    "    for i in range(0, len(url_list)):\n",
    "        #set header\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "          'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "          'Accept-Encoding': 'none',\n",
    "          'Accept-Language': 'en-US,en;q=0.8',\n",
    "          'Connection': 'keep-alive'}\n",
    "        #open url and parse result \n",
    "        page = requests.get(url_list[i], headers=hdr)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #write result to disk\n",
    "        with open(fp_indiv_page+\"/craigslist_car_page\"+str(i+1)+'.html', \"w\", encoding = 'utf-8') as file:\n",
    "                    # prettify the soup object and convert it into a string  \n",
    "                    file.write(str(soup))\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa73340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get all attributes from individual pages\n",
    "def get_att(collection, fp_indiv_page, num_page, fp_full_page):\n",
    "    #call individual page function\n",
    "    get_individual_page(num_page, fp_indiv_page, fp_full_page)\n",
    "    #set client\n",
    "    mo_c = MongoClient()\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    #create new database\n",
    "    db = client[\"craigslist_cars\"]\n",
    "    #set desired collection\n",
    "    collection=collection\n",
    "    #get the url list\n",
    "    url_list=get_page_url(num_page, fp_full_page)\n",
    "    print('got url list')\n",
    "    #set brand list of 24 brands\n",
    "    brand_list=['audi', 'ford', 'chevy',\n",
    "               'lexus','nissan','dodge','chevrolet','mazda',\n",
    "               'acura','volkswagen','mercedes','gmc','kia','honda',\n",
    "               'porsche','infiniti','scion','jeep','vw','subaru','mitsubishi',\n",
    "               'toyota','hyundai','tesla','fiat','mini cooper','infiniti']\n",
    "    print('starting loop')\n",
    "    #loop through url list\n",
    "    for i in range(0, len(url_list)):\n",
    "        #create empty dic\n",
    "        dic={}\n",
    "        #get url\n",
    "        dic['url']=url_list[i]\n",
    "        #open indiv page and convert to soup object\n",
    "        html_file=open(fp_indiv_page+\"/craigslist_car_page\"+str(i+1)+\".html\", \"r\").read()\n",
    "        soup=BeautifulSoup(html_file, 'html.parser')\n",
    "        #set empty attributes list \n",
    "        attributes=[]\n",
    "        #loop through all attribute objects\n",
    "        for att in soup.find_all('p',{'class':'attrgroup'}):\n",
    "            for d in att.find_all('span'):\n",
    "                #append to list\n",
    "                attributes.append(d.text)\n",
    "        print(attributes)\n",
    "        #set function to retrieve attribute values from previous list\n",
    "        def get_attribute(el):\n",
    "            return [i for i in attributes if el in i][0]\n",
    "        #try and except statements to account for situations where the brand isn't present\n",
    "        try:\n",
    "            #get full listing title\n",
    "            full_car_brand=soup.find('span',{'id':'titletextonly'}).text\n",
    "            dic['full_car_brand']=full_car_brand\n",
    "            title=full_car_brand.lower()\n",
    "            #get year with regex\n",
    "            dic['year']=float(re.findall(r'.*([1-3][0-9]{3})', title)[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #find if any of the brands are in the title\n",
    "            dic['brand']=[brand for brand in brand_list if(brand in title)][0]\n",
    "        except:\n",
    "            dic['brand']=np.nan\n",
    "\n",
    "        try:\n",
    "            #find fuel attribute\n",
    "            fuel=get_attribute('fuel')\n",
    "            #clean up valie\n",
    "            fuel=fuel.replace('fuel: ','')\n",
    "            dic['fuel']=fuel\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get odometer value and clean up\n",
    "            odo=get_attribute('odometer')\n",
    "            odo=float(odo.replace('odometer: ',''))\n",
    "            dic['odo']=float(odo)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get condition value and clean up\n",
    "            cond=get_attribute('condition')\n",
    "            cond=cond.replace('condition: ','')\n",
    "            dic['cond']=cond\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get paint value and clean up\n",
    "            paint=get_attribute('paint color')\n",
    "            paint=paint.replace('paint color: ','')\n",
    "            dic['paint']=paint\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get size value and clean it up\n",
    "            size=get_attribute('size')\n",
    "            size=size.replace('size: ','')\n",
    "            dic['size']=size\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get cylinders value and clean it up\n",
    "            cyl=get_attribute('cylinders')\n",
    "            cyl=cyl.replace('cylinders','')\n",
    "            cyl=float(cyl.replace(':',''))\n",
    "            dic['cyl']=cyl\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get title value and clean it up\n",
    "            title=get_attribute('title')\n",
    "            title=title.replace('title status: ','')\n",
    "            dic['title']=title\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get transmission value and clean it up\n",
    "            trans=get_attribute('transmission')\n",
    "            trans=trans.replace('transmission: ','')\n",
    "            dic['trans']=trans\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get drive value and clean it up\n",
    "            drive=get_attribute('drive')\n",
    "            drive=drive.replace('drive: ','')\n",
    "            dic['drive']=drive\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get body type value and clean it up\n",
    "            body_type=get_attribute('type')\n",
    "            body_type=body_type.replace('type: ','')\n",
    "            dic['body_type']=body_type\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get image url\n",
    "            dic['img_url']=soup.find('img').get('src')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #get price and clean it up\n",
    "            price=soup.find('span',{'class':'price'}).text\n",
    "            price=price.replace('$','')\n",
    "            price=price.replace(',','')\n",
    "            dic['price']=float(price)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            #empty dic for nested documents\n",
    "            time_dic={}\n",
    "            object_id=[]\n",
    "            #find all time values\n",
    "            for i in soup.find_all('time',{'class':'date timeago'}):\n",
    "                object_id.append(i.text.split())\n",
    "            #convert to datetime and append to dictionary\n",
    "            time_dic['posted_time']=pd.to_datetime(object_id[0][1])\n",
    "            time_dic['last_updated_time']=pd.to_datetime(object_id[1][1])\n",
    "            #append whole dictionary to outer dictionary\n",
    "            dic['posted_stats']=time_dic\n",
    "        except:\n",
    "            pass\n",
    "        #insert a value to collection\n",
    "        collection.insert_one(dic)\n",
    "        print(i)\n",
    "        print(dic)\n",
    "    #set index\n",
    "    collection.create_index('brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aab53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new collection 'bayc'\n",
    "mo_c = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "#create new database\n",
    "db = client[\"craigslist_cars\"]\n",
    "collection_bay = db[\"bay_area\"]\n",
    "collection_sd=db['sd']\n",
    "collection_la=db['los_angeles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b03811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#num_page=72 for SF\n",
    "#num_page=33 for San Diego\n",
    "#num_page=61 for Los Angeles\n",
    "#region is sfbay, sandiego, and losangeles\n",
    "if __name__ == '__main__':\n",
    "    scrape_full_page('losangeles', 61,'full_page_la')\n",
    "    get_att(collection_la, 'individual_page_la', 61, 'full_page_la')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b153c3",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This code takes a long time to run. I have provided the full result of the scrape, feel free to verify that it worked with the provided files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
